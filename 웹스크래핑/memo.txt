웹크롤링? 웹스크래핑!

* 웹
html 뼈대 
css 예쁘게
javascript 살아있게

HTML (Hyper Text Markup Language)

1. html
2. XPath
[SELECTOR] XPATH와 CSS Selector 차이 
CSS seletor는 원하는 범위의 값을 가진 태그'를 선택하는 선택자가 없다. 
예를 들어, 'value가 50이상인 input태그를 선택'한다고 했을 때 방법이 없다. 
그냥 a태그 전체를 탐색해서 50이상인 태그만 스타일을 적용해주는 스크립트를 작성해야만 했다. 
또, 선택한 태그들의 값을 바로 뽑아서 사용하고 싶어도 map으로 한번 가공해줘야했다.

CSS Selector가 하지못했던 일을 XPATH는 할 수 있다. 
"//input[value>=50]". 매우 간단한 방법으로 처리한다. 또한, 
선택된 태그의 값만 가지고 오려면 "//input[value>=50]/@value" 이렇게 사용하면 된다.
이 외에도 4번째에 있는 자식 태그를 탐색하기위해 "div:nth-child(4)"와 같이 복잡한 문법이 아니라, 
"//div/*[4]" 간결한 문법을 제공한다.

출처- https://heodolf.tistory.com/7

3. 크롬
4. Requets 라이브러리 install
터미널 > pip install requests 

import requests

5. 정규식 기본 (Regular expression)

import re

1. p = re.compile ("원하는 형태")
2. m = p.match("비교할 문자열") : 주어진 문자열의 처음부터 일치하는지 확인
3. m = p.search("비교할 문자열) : 주어진 문자열 중에 일치하는게 있는지 확인
4. lst = p.findall("비교할 문자열) : 일치하는 모든 것을 "리스트" 형태로 반환

 "원하는 형태" : 정규식
 . (ca.e) : 하나의 문자를 의미 > care, cafe, case (o) | caffe (x)
 ^ (^de) : 문자열의 시작 > dest, destination (o) | fade (x)
 $ (se$) : 문자열의 끝 > case, base (o) | face (x)

[참고]
https://www.w3schools.com/python/python_regex.asp
https://docs.python.org/3/library/re.html 

6. User Agent (사용자를 대신하여 일을 수행하는 소프트웨어 에이전트)
접속하는 브라우저에 따라 User Agent가 다르다.

7. BeaurifulSoup4
터미널 >
1. pip install beautifulsoup4, 
2. pip install lxml (parser 구문분석)

[참고] (한글번역본)
https://www.crummy.com/software/BeautifulSoup/bs4/doc.ko/

* Parsing (파싱) 이란? Parser (파서) 란?

- Parsing (파싱)
  언어학에서 parsing은 구문 분석이라고도하며 문장을 그것을 이루고 있는 구성 성분으로 분해하고 
  그들 사이의 위계 관계를 분석하여 문장의 구조를 결정하는 것

- Parser (파서)
  parsing을 하는 processor 즉 parser가 parsing을 하는 것 (parsing을 수행하는 프로그램)
  parser란 compiler의 일부로 컴파일러나 인터프리터에서 원시 프로그램을 읽어 들여 
  그 문장의 구조를 알아내는 parsing(구문 분석)을 행하는 프로그램

출처 - https://na27.tistory.com/230

* 파이썬 인터프리터(python interpreter)란?
- 컴파일 언어 : 소스 코드를 기계어로 컴파일해서 실행파일을 만들고 실행.
- 인터프러터 언어 : 코드를 한 줄씩 읽어 내려가며 실행하는 프로그래밍 언어. 

- [ 장점 ]
  배우기 쉬워서 학습용으로 좋다.  
  공동 작업과 유지 보수가 아주 쉽고 편해서 생산성이 높고 실사용률도 높다.
  읽고 쓰기 쉽다.

- [ 단점 ]
  느리다.

cf. 자바 .Java ----> 컴파일 ----> class ---> 인터프린터 ----> 실행 (하이브리드?)

출처 - https://gusdnr69.tistory.com/55

* HTTP METHOD
- GET
- POST : HTTP 메시지 body에 숨겨서 보냄. 보안, 길이제한x

★ 스크래핑 tip.
페이지별 url, 개발자도구 (구조)

8. CSV 파일 (Comma Separated Values)
- 표 형태의 데이터를 저장하는 파일 형식.
- 한 줄이 한 개의 행에 해당하며, 열 사이에는 쉼표(,)를 넣어 구분한다.

☆ CSV 파일 쓰기
  CSV 파일을 쓰기 위해서는 .csv 파일을 쓰기모드로 오픈하고 파일객체를 csv.writer(파일객체) 에 넣으면 된다.
  CSV writer는 writerow() 라는 메서드를 통해 list 데이타를 한 라인 추가하게 된다.
  윈도우즈의 경우 csv 모듈에서 데이타를 쓸 때 각 라인 뒤에 빈 라인이 추가되는 문제가 있는데, 
  이를 없애기 위해 (파이썬 3 에서) 파일을 open 할 때 newline='' 와 같은 옵션을 지정한다 
  아래 예제는 output.csv 라는 CSV 파일에 2개 라인을 추가하는 예이다.

  import csv    
  f = open('output.csv', 'w', encoding='utf-8-sig', newline='')
  wr = csv.writer(f)
  wr.writerow([1, "김정수", False])
  wr.writerow([2, "박상미", True])
  f.close()

출처 - http://pythonstudy.xyz/python/article/207-CSV-%ED%8C%8C%EC%9D%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0

9. Selenium
터미널 > pip install selenium
크롬 드라이브 다운로드 (PYTHONWORKSPACE) ★ 크롬 버전이랑 일치해야함!!

from selenium import webdriver

*셀레니움 활용
- 네이버 로그인
- 네이버 항공권 (loding오류 해결)
- 구글무비 * 동적페이지 웹스크래핑 (사용자가 어떤 동작을 했을때 동작)

☆ loding시간으로 인한 오류 해결 
  ex) no such element...

1. import time 
   time.sleep(3)

>> 3초 기다렸다가 실행
>> 비효율

2. from selenium.webdriver.common.by import By
   from selenium.webdriver.support.ui import WebDriverWait
   from selenium.webdriver.support import expected_conditions as EC

   try:
      elem = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, "//*[@id='content']/div[2]/div/div[4]/ul/li[1]"))) # ※괄호2개 
      # 성공했을 때 동작 수행
   finally:
      browser.quit()
      # 실패했을경우 브라우저 닫음

>> WebDriverWait를 통해서 브라우저 (최대)10초동안 기다린다.
>> EC.presence_of_element_located ~> By.XPATH라는 조건으로 해당하는 엘리먼트가 위치할때까지.
>> (로딩이 얼마나 걸릴지 모르기 때문에 좀더 효율적)

10. Headless 크롬
*셀레니움 - 매번 브라우저를 띄움 번거롭고, 느리다 >>Headless 크롬
(크롬을 띄우지 않고 크롬을 쓸 수 있음)

from selenium import webdriver

★ headless 크롬 설정
options = webdriver.ChromeOptions()
options.headless = True
options.add_argument("window-size=1920x1080")

browser = webdriver.Chrome(options=options)
browser.maximize_window

>> ☆ 스크린샷을 통해서 원하는 동작을 했는지 확인가능
browser.get_screenshot_as_file("google_movie.png") 

>> headless 크롬 사용시 문제점
headless 크롬일 경우 따로 user-agent 설정 안해주면 
HeadlessChrome 정보가 user-agent에 들어가기  때문에 브라우저 접속 막힐 수 있음

*문제해결 : user-agent 설정
options.add_argument("user-agent=나의 user-agent 복붙")